# model/train_model.py
import os
import pandas as pd
import sys
# Add the parent directory to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', 'app')))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..','..','..')))

import joblib
from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import roc_auc_score, classification_report
from app.config.config import MODEL_PATH
import logging

def train_and_save_model(df: pd.DataFrame):
    # 1. Parse datetime safely
    def safe_parse_datetime(series, column_name):
        try:
            return pd.to_datetime(series, format="%Y-%m-%d %H:%M:%S", errors="coerce")
        except Exception as e:
            print(f"[!] Error parsing {column_name}: {e}")
            return pd.to_datetime(series, errors="coerce")

    df["booking_time"] = pd.to_datetime(df["booking_time"], errors="coerce")
    df["reservation_datetime"] = pd.to_datetime(df["reservation_date"] + " " + df["reservation_time"], errors="coerce")

    logging.info(f"[DEBUG] Rows before dropna: {len(df)}")
    df.dropna(subset=["booking_time", "reservation_datetime"], inplace=True)
    logging.info(f"[DEBUG] Rows after dropna: {len(df)}")

    df["booking_time"] = df["booking_time"].dt.tz_localize(None)


    # 2. Feature engineering
    try:
        logging.info("Sample row:\n%s", df.iloc[0].to_string())
        logging.info("Dtypes before feature engineering:\n%s", df.dtypes.to_string())

        logging.info("➡ Creating 'booking_hour'")
        df["booking_hour"] = df["booking_time"].dt.hour

        logging.info("➡ Creating 'reservation_hour'")
        df["reservation_hour"] = df["reservation_datetime"].dt.hour  

        logging.info("➡ Creating 'advance_minutes'")
        df["advance_minutes"] = (df["reservation_datetime"] - df["booking_time"]).dt.total_seconds() / 60

        logging.info("➡ Creating 'day_of_week'")
        df["day_of_week"] = df["reservation_datetime"].dt.weekday

        logging.info("➡ Creating 'is_weekend'")
        df["is_weekend"] = df["day_of_week"].isin([5, 6]).astype(int)

        logging.info("✅ Feature engineering completed successfully")

    except Exception as e:
        logging.error("❌ Error during feature engineering: %s", str(e))
        logging.error("Data types:\n%s", df.dtypes.to_string())
        logging.error("Sample row on error:\n%s", df.iloc[0].to_string())


    # 3. Features & label
    FEATURES = [
         "booking_hour", "reservation_hour", "advance_minutes",
        "num_guests", "is_first_booking", "day_of_week", "is_weekend",
         "avg_user_cancel_rate","payment_status","user_distance_km", "total_cancel_bookings","total_bookings"
    ]
    TARGET = "is_arrival"


    X = df[FEATURES]
    y = df[TARGET]
    logging.info("One row of features (X):\n%s", X.iloc[0].to_string())

    # 4. Train/test split
    logging.info("Train/test split")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

    # 5. Preprocessing
    logging.info("Preprocessing")
    numeric_features = [
        "booking_hour", "reservation_hour", "advance_minutes",
        "num_guests", "is_first_booking", "day_of_week",
        "is_weekend", "avg_user_cancel_rate","user_distance_km","total_cancel_bookings","total_bookings"
    ]

    categorical_features = ["payment_status"]

    numeric_transformer = StandardScaler()
    categorical_transformer = OneHotEncoder(handle_unknown="ignore", sparse_output=False)

    preprocessor = ColumnTransformer(
        transformers=[
            ("num", numeric_transformer, numeric_features),
            ("cat", categorical_transformer, categorical_features)
        ]
    )

    # 6. Pipeline
    logging.info("Pipeline")
    pipeline = Pipeline([
        ("prep", preprocessor),
        ("clf", RandomForestClassifier(n_estimators=200, random_state=42, class_weight="balanced"))
    ])

    # 7. Grid Search
    logging.info("Grid Search")
    param_grid = {
        "clf__max_depth": [5, 10, None],
        "clf__min_samples_split": [2, 5, 10],
    }
    grid = GridSearchCV(pipeline, param_grid, cv=3, scoring="roc_auc", n_jobs=-1)
    grid.fit(X_train, y_train)
        


    # 8. Evaluate
    logging.info("Evaluate")
    best_model = grid.best_estimator_
    y_pred_proba = best_model.predict_proba(X_test)[:, 1]
    print("ROC AUC:", roc_auc_score(y_test, y_pred_proba))
    print(classification_report(y_test, best_model.predict(X_test)))

    # 9. Save model
    logging.info("Save model")
    logging.info("Save model")
    print(f"[DEBUG] Current working directory: {os.getcwd()}")
    print(f"[DEBUG] Saving model to: {MODEL_PATH}")

    try:
        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        joblib.dump(best_model, MODEL_PATH, compress=3)
        logging.info(f"[✔] Model saved to {MODEL_PATH}")
    except Exception as e:
        logging.error(f"❌ Failed to save model: {e}")
        print(f"[✘] Failed to save model: {e}")
    
    
        # 10. Feature Importance Analysis
    import matplotlib.pyplot as plt
    import numpy as np
    # import shap
    from sklearn.inspection import permutation_importance

    # Lấy lại tên các cột sau khi preprocessing (numeric + one-hot encoded)
    final_feature_names = (
        numeric_features +
        list(best_model.named_steps["prep"]
             .transformers_[1][1]  # categorical transformer
             .get_feature_names_out(categorical_features))
    )

    # 1. Built-in Feature Importance
    importances = best_model.named_steps["clf"].feature_importances_
    sorted_idx = np.argsort(importances)[::-1]

    plt.figure(figsize=(10, 6))
    plt.title("Built-in Feature Importance (Gini)")
    plt.barh(np.array(final_feature_names)[sorted_idx], importances[sorted_idx])
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig("built_in_importance.png")
    print("✔ Saved built-in feature importance to built_in_importance.png")

    # 2. Permutation Feature Importance
    print("Computing permutation importance...")
    result = permutation_importance(best_model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)
    sorted_idx_perm = result.importances_mean.argsort()[::-1]

    plt.figure(figsize=(10, 6))
    plt.title("Permutation Feature Importance")
    plt.barh(np.array(final_feature_names)[sorted_idx_perm], result.importances_mean[sorted_idx_perm])
    plt.gca().invert_yaxis()
    plt.tight_layout()
    plt.savefig("permutation_importance.png")
    print("✔ Saved permutation importance to permutation_importance.png")

    # # 3. SHAP Values (KernelExplainer for pipeline)
    # print("Computing SHAP values...")
    # # Lấy pipeline xử lý dữ liệu
    # X_sample = X_test.sample(n=100, random_state=42)  # SHAP nên dùng sample nhỏ
    # explainer = shap.Explainer(best_model.named_steps["clf"],
    #                            best_model.named_steps["prep"].transform(X_sample),
    #                            feature_names=final_feature_names)
    # shap_values = explainer(best_model.named_steps["prep"].transform(X_sample))

    # plt.figure()
    # shap.plots.beeswarm(shap_values, max_display=20, show=False)
    # plt.title("SHAP Feature Importance")
    # plt.tight_layout()
    # plt.savefig("shap_importance.png")
    # print("✔ Saved SHAP feature importance to shap_importance.png")
    from treeinterpreter import treeinterpreter as ti
    best_model = grid.best_estimator_

    # Lấy 5 mẫu từ tập test
    X_sample = X_test[:5]
    y_sample = y_test[:5]

    # Lấy model đã huấn luyện (clf trong pipeline)
    clf = best_model.named_steps["clf"]
    X_transformed = best_model.named_steps["prep"].transform(X_sample)

    # Giải thích bằng treeinterpreter
    predictions, bias, contributions = ti.predict(clf, X_transformed)

    for i in range(len(X_sample)):
        print(f"\n➡ Dự đoán cho mẫu {i+1}:")
        print(f"  - Prediction: {predictions[i]}")
        print(f"  - Bias (gốc từ dữ liệu huấn luyện): {bias[i]}")
        print(f"  - Feature contributions:")
        for name, value in zip(final_feature_names, contributions[i]):
            print(f"     {name}: {value}")
if __name__ == "__main__":
    # Load the CSV data file
    # df = pd.read_csv(r'C:\Users\LENOVO\Desktop\src_DACN\DACN_HK241\BE\AI_Service\data\sample_reservation_data.csv')
    df = pd.read_csv('/app/data/sample_reservation_data.csv')


    # Call the function to train and save the model
    train_and_save_model(df)